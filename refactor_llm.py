#!/usr/bin/env python3
"""
LLM í´ë¼ì´ì–¸íŠ¸ ë¦¬íŒ©í† ë§ ìŠ¤í¬ë¦½íŠ¸
í”„ë¡¬í”„íŠ¸ ë¡œì§ì„ Base í´ë˜ìŠ¤ë¡œ í†µí•©í•˜ê³ , ê° í´ë¼ì´ì–¸íŠ¸ëŠ” API í˜¸ì¶œë§Œ ë‹´ë‹¹
"""

import os
from pathlib import Path

# Base í´ë˜ìŠ¤ ì½”ë“œ
BASE_CODE = '''"""
LLM í´ë¼ì´ì–¸íŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤
"""
import logging
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)


class PromptBuilder:
    """í”„ë¡¬í”„íŠ¸ êµ¬ì„± ìœ í‹¸ë¦¬í‹° - ëª¨ë“  LLM í´ë¼ì´ì–¸íŠ¸ì—ì„œ ê³µí†µ ì‚¬ìš©"""
    
    @staticmethod
    def build_system_prompt() -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return """ë‹¹ì‹ ì€ ì œí’ˆ ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
êµ¬ë§¤ìë“¤ì˜ í›„íšŒ ìš”ì¸ì„ ë¶„ì„í•˜ì—¬ ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.
ì¹œê·¼í•˜ì§€ë§Œ ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ, JSON í˜•ì‹ì˜ êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."""
    
    @staticmethod
    def build_user_prompt(
        top_factors: List[tuple],
        evidence_reviews: List[Dict[str, Any]],
        total_turns: int,
        category_name: str,
        product_name: str,
        dialogue_history: Optional[List[Dict[str, str]]] = None
    ) -> str:
        """ìœ ì € í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        # ìƒìœ„ ìš”ì¸ ì •ë¦¬
        factors_text = "\\n".join([
            f"{i+1}. {factor_key} (ì ìˆ˜: {score:.2f})"
            for i, (factor_key, score) in enumerate(top_factors[:5])
        ])
        
        # ëŒ€í™” ë‚´ìš© ì •ë¦¬
        dialogue_text = ""
        if dialogue_history:
            dialogue_lines = []
            for turn in dialogue_history:
                role = turn.get('role', '')
                text = turn.get('message', '')
                if role == 'user':
                    dialogue_lines.append(f"ì‚¬ìš©ì: {text}")
                elif role == 'assistant':
                    dialogue_lines.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {text}")
            dialogue_text = "\\n".join(dialogue_lines)
        
        # ì¦ê±° ë¦¬ë·° ì •ë¦¬
        evidence_text = ""
        for i, rev in enumerate(evidence_reviews, 1):
            label = rev.get('label', 'NEU')
            rating = rev.get('rating', 0)
            excerpt = rev.get('excerpt', '')
            evidence_text += f"{i}. [{label}] {rating}ì  - {excerpt}\\n"
        
        # User Prompt êµ¬ì„±
        user_prompt_parts = [
            "**ì œí’ˆ ì •ë³´**",
            f"- ì¹´í…Œê³ ë¦¬: {category_name}",
            f"- ì œí’ˆëª…: {product_name}",
            f"- ë¶„ì„ ëŒ€í™” í„´: {total_turns}í„´",
            ""
        ]
        
        if dialogue_text:
            user_prompt_parts.extend([
                "**ëŒ€í™” ë‚´ìš©**",
                dialogue_text,
                ""
            ])
        
        user_prompt_parts.extend([
            "**ì£¼ìš” í›„íšŒ ìš”ì¸ (ìƒìœ„ 5ê°œ)**",
            factors_text,
            "",
            f"**ì¦ê±° ë¦¬ë·° ì „ì²´ ({len(evidence_reviews)}ê°œ)**",
            evidence_text,
            "",
            "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ìµœì¢… ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:",
            "{",
            '  "summary": "í•µì‹¬ í›„íšŒ ìš”ì¸ ì„¤ëª… (2-3ë¬¸ì¥)",',
            '  "key_findings": [',
            '    {',
            '      "factor": "ìš”ì¸ëª…",',
            '      "risk_level": "high|mid|low",',
            '      "what_users_say": "êµ¬ë§¤ìë“¤ì´ ë§í•˜ëŠ” ë‚´ìš© (2-3ë¬¸ì¥)"',
            '    }',
            '  ],',
            '  "balanced_view": {',
            '    "pros": [{"point": "ì¥ì "}],',
            '    "cons": [{"point": "ë‹¨ì "}],',
            '    "mixed": [{"point": "ìƒí™©ì— ë”°ë¼ ë‹¤ë¦„"}]',
            '  },',
            '  "decision_rule": {',
            '    "if_buy": ["êµ¬ë§¤ë¥¼ ê³ ë ¤í•´ë„ ì¢‹ì€ ê²½ìš°"],',
            '    "if_hold": ["ë³´ë¥˜ê°€ ë‚˜ì€ ê²½ìš°"]',
            '  },',
            '  "final_recommendation": "êµ¬ë§¤|ë³´ë¥˜|ì¡°ê±´ë¶€ ì¶”ì²œ",',
            '  "one_line_tip": "í•œ ì¤„ ì¡°ì–¸"',
            "}",
            "",
            "**ì¤‘ìš”**: ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ë§ˆí¬ë‹¤ìš´ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."
        ])
        
        return "\\n".join(user_prompt_parts)
    
    @staticmethod
    def get_fallback_summary(
        top_factors: List[tuple],
        category_name: str,
        product_name: str
    ) -> str:
        """API ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ìš”ì•½"""
        factors_text = ", ".join([f"{key}" for key, _ in top_factors[:3]])
        
        return f"""{{
  "summary": "{product_name}ì˜ ì£¼ìš” í›„íšŒ ìš”ì¸ì€ {factors_text}ì…ë‹ˆë‹¤.",
  "key_findings": [
    {{
      "factor": "{top_factors[0][0] if top_factors else 'ì•Œ ìˆ˜ ì—†ìŒ'}",
      "risk_level": "mid",
      "what_users_say": "êµ¬ë§¤ìë“¤ì´ ì´ ë¶€ë¶„ì—ì„œ ì•„ì‰¬ì›€ì„ ëŠë¼ê³  ìˆìŠµë‹ˆë‹¤."
    }}
  ],
  "balanced_view": {{
    "pros": [{{"point": "ì „ë°˜ì ì¸ í’ˆì§ˆì€ ì–‘í˜¸í•©ë‹ˆë‹¤"}}],
    "cons": [{{"point": "{factors_text} ê´€ë ¨ ë¶ˆë§Œì´ ìˆìŠµë‹ˆë‹¤"}}],
    "mixed": []
  }},
  "decision_rule": {{
    "if_buy": ["í•´ë‹¹ ìš”ì¸ì´ ë³¸ì¸ì—ê²Œ ì¤‘ìš”í•˜ì§€ ì•Šì€ ê²½ìš°"],
    "if_hold": ["í•´ë‹¹ ìš”ì¸ì´ ë³¸ì¸ì—ê²Œ ì¤‘ìš”í•œ ê²½ìš°"]
  }},
  "final_recommendation": "ì¡°ê±´ë¶€ ì¶”ì²œ",
  "one_line_tip": "í›„íšŒ ìš”ì¸ì„ ë¯¸ë¦¬ ì•Œê³  êµ¬ë§¤í•˜ë©´ ì‹¤ë§ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
}}"""


class BaseLLMClient(ABC):
    """LLM í´ë¼ì´ì–¸íŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤ - í…œí”Œë¦¿ ë©”ì„œë“œ íŒ¨í„´ ì‚¬ìš©"""
    
    def __init__(self, api_key: str, model: str, temperature: float, max_tokens: int):
        self.api_key = api_key
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
    
    def generate_summary(
        self, 
        top_factors: List[tuple],
        evidence_reviews: List[Dict[str, Any]],
        total_turns: int,
        category_name: str,
        product_name: str = "ì´ ì œí’ˆ",
        dialogue_history: Optional[List[Dict[str, str]]] = None
    ) -> str:
        """
        ìµœì¢… ë¶„ì„ ìš”ì•½ ìƒì„± (í…œí”Œë¦¿ ë©”ì„œë“œ)
        
        Args:
            top_factors: [(factor_key, score), ...] ìƒìœ„ í›„íšŒ ìš”ì¸
            evidence_reviews: ì¦ê±° ë¦¬ë·° ë¦¬ìŠ¤íŠ¸
            total_turns: ì´ ëŒ€í™” í„´ ìˆ˜
            category_name: ì œí’ˆ ì¹´í…Œê³ ë¦¬ëª…
            product_name: ì œí’ˆëª…
            dialogue_history: ëŒ€í™” ë‚´ì—­
            
        Returns:
            str: ìƒì„±ëœ ë¶„ì„ ìš”ì•½ (JSON í˜•ì‹)
        """
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê³µí†µ ë¡œì§)
        system_prompt = PromptBuilder.build_system_prompt()
        user_prompt = PromptBuilder.build_user_prompt(
            top_factors, evidence_reviews, total_turns,
            category_name, product_name, dialogue_history
        )
        
        # í”„ë¡¬í”„íŠ¸ ì €ì¥ (OpenAIë§Œ)
        if self.__class__.__name__ == "OpenAIClient":
            self._save_prompt(system_prompt, user_prompt)
        
        # API í˜¸ì¶œ (ê° êµ¬í˜„ì²´ì—ì„œ ì •ì˜)
        try:
            return self._call_api(system_prompt, user_prompt)
        except Exception as e:
            logger.error(f"{self.__class__.__name__} API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return PromptBuilder.get_fallback_summary(top_factors, category_name, product_name)
    
    @abstractmethod
    def _call_api(self, system_prompt: str, user_prompt: str) -> str:
        """
        ì‹¤ì œ LLM API í˜¸ì¶œ (ê° êµ¬í˜„ì²´ì—ì„œ êµ¬í˜„)
        
        Args:
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            user_prompt: ìœ ì € í”„ë¡¬í”„íŠ¸
            
        Returns:
            str: LLM ì‘ë‹µ
        """
        pass
    
    def _save_prompt(self, system_prompt: str, user_prompt: str):
        """í”„ë¡¬í”„íŠ¸ ì €ì¥ (OpenAI ì „ìš©, ë‹¤ë¥¸ í´ë¼ì´ì–¸íŠ¸ëŠ” override ë¶ˆí•„ìš”)"""
        pass
'''

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì½”ë“œ
OPENAI_CODE = '''"""
OpenAI LLM í´ë¼ì´ì–¸íŠ¸
"""
import logging
from .llm_base import BaseLLMClient

logger = logging.getLogger(__name__)


class OpenAIClient(BaseLLMClient):
    """OpenAI API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini", temperature: float = 0.7, max_tokens: int = 2000):
        super().__init__(api_key, model, temperature, max_tokens)
        
        if not api_key:
            logger.warning("OpenAI API keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.client = None
        else:
            try:
                from openai import OpenAI
                self.client = OpenAI(api_key=api_key)
                logger.info(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: model={model}")
            except Exception as e:
                logger.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.client = None
    
    def _call_api(self, system_prompt: str, user_prompt: str) -> str:
        """OpenAI API í˜¸ì¶œ"""
        if not self.client:
            raise RuntimeError("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        
        summary = response.choices[0].message.content.strip()
        logger.info(f"OpenAI ìš”ì•½ ìƒì„± ì™„ë£Œ: {len(summary)}ì")
        return summary
    
    def _save_prompt(self, system_prompt: str, user_prompt: str):
        """í”„ë¡¬í”„íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            from datetime import datetime
            from pathlib import Path
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            out_dir = Path("out")
            out_dir.mkdir(exist_ok=True)
            
            prompt_file = out_dir / f"llm_prompt_{timestamp}.txt"
            with open(prompt_file, "w", encoding="utf-8") as f:
                f.write("=" * 80 + "\\n")
                f.write("SYSTEM PROMPT\\n")
                f.write("=" * 80 + "\\n")
                f.write(system_prompt)
                f.write("\\n\\n")
                f.write("=" * 80 + "\\n")
                f.write("USER PROMPT\\n")
                f.write("=" * 80 + "\\n")
                f.write(user_prompt)
                f.write("\\n")
            
            logger.info(f"[LLM í”„ë¡¬í”„íŠ¸ ì €ì¥] {prompt_file}")
        except Exception as e:
            logger.error(f"í”„ë¡¬í”„íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
'''

# Gemini í´ë¼ì´ì–¸íŠ¸ ì½”ë“œ
GEMINI_CODE = '''"""
Gemini LLM í´ë¼ì´ì–¸íŠ¸
"""
import logging
from .llm_base import BaseLLMClient

logger = logging.getLogger(__name__)


class GeminiClient(BaseLLMClient):
    """Google Gemini API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, api_key: str, model: str = "gemini-1.5-flash", temperature: float = 0.7, max_tokens: int = 2000):
        super().__init__(api_key, model, temperature, max_tokens)
        
        if not api_key:
            logger.warning("Gemini API keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.client = None
        else:
            try:
                import google.generativeai as genai
                genai.configure(api_key=api_key)
                self.client = genai.GenerativeModel(model)
                logger.info(f"Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: model={model}")
            except Exception as e:
                logger.error(f"Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.client = None
    
    def _call_api(self, system_prompt: str, user_prompt: str) -> str:
        """Gemini API í˜¸ì¶œ"""
        if not self.client:
            raise RuntimeError("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # GeminiëŠ” system_promptì™€ user_promptë¥¼ í•©ì³ì„œ ì „ë‹¬
        combined_prompt = f"{system_prompt}\\n\\n{user_prompt}"
        
        response = self.client.generate_content(
            combined_prompt,
            generation_config={
                "temperature": self.temperature,
                "max_output_tokens": self.max_tokens,
            }
        )
        
        summary = response.text.strip()
        logger.info(f"Gemini ìš”ì•½ ìƒì„± ì™„ë£Œ: {len(summary)}ì")
        return summary
'''

# Claude í´ë¼ì´ì–¸íŠ¸ ì½”ë“œ
CLAUDE_CODE = '''"""
Anthropic Claude LLM í´ë¼ì´ì–¸íŠ¸
"""
import logging
from .llm_base import BaseLLMClient

logger = logging.getLogger(__name__)


class ClaudeClient(BaseLLMClient):
    """Anthropic Claude API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, api_key: str, model: str = "claude-3-5-sonnet-20241022", temperature: float = 0.7, max_tokens: int = 2000):
        super().__init__(api_key, model, temperature, max_tokens)
        
        if not api_key:
            logger.warning("Anthropic API keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.client = None
        else:
            try:
                from anthropic import Anthropic
                self.client = Anthropic(api_key=api_key)
                logger.info(f"Claude í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: model={model}")
            except Exception as e:
                logger.error(f"Claude í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.client = None
    
    def _call_api(self, system_prompt: str, user_prompt: str) -> str:
        """Claude API í˜¸ì¶œ"""
        if not self.client:
            raise RuntimeError("Claude í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        response = self.client.messages.create(
            model=self.model,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            system=system_prompt,
            messages=[
                {"role": "user", "content": user_prompt}
            ]
        )
        
        summary = response.content[0].text.strip()
        logger.info(f"Claude ìš”ì•½ ìƒì„± ì™„ë£Œ: {len(summary)}ì")
        return summary
'''

def main():
    """ë¦¬íŒ©í† ë§ ì‹¤í–‰"""
    llm_dir = Path(__file__).parent / "backend" / "llm"
    
    # íŒŒì¼ ì‘ì„±
    files = {
        "llm_base.py": BASE_CODE,
        "llm_openai.py": OPENAI_CODE,
        "llm_gemini.py": GEMINI_CODE,
        "llm_claude.py": CLAUDE_CODE
    }
    
    for filename, code in files.items():
        filepath = llm_dir / filename
        print(f"âœï¸  {filename} ì‘ì„± ì¤‘...")
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(code)
        print(f"âœ… {filename} ì™„ë£Œ")
    
    print("\nğŸ‰ LLM í´ë¼ì´ì–¸íŠ¸ ë¦¬íŒ©í† ë§ ì™„ë£Œ!")
    print("ğŸ“ ë³€ê²½ ì‚¬í•­:")
    print("  - PromptBuilder í´ë˜ìŠ¤ë¥¼ llm_base.pyì— ì¶”ê°€")
    print("  - ê° LLM í´ë¼ì´ì–¸íŠ¸ëŠ” _call_api() ë©”ì„œë“œë§Œ êµ¬í˜„")
    print("  - í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë¡œì§ì´ ì¤‘ë³µ ì œê±°ë¨")
    print("  - í…œí”Œë¦¿ ë©”ì„œë“œ íŒ¨í„´ ì ìš©")

if __name__ == "__main__":
    main()
