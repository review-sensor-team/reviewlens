"""
Gemini LLM ν΄λΌμ΄μ–ΈνΈ
"""
import logging
from typing import List, Dict, Any, Optional
from .llm_base import BaseLLMClient

logger = logging.getLogger(__name__)


class GeminiClient(BaseLLMClient):
    """Google Gemini API ν΄λΌμ΄μ–ΈνΈ"""
    
    def __init__(self, api_key: str, model: str = "gemini-1.5-flash", temperature: float = 0.7, max_tokens: int = 2000):
        self.api_key = api_key
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        if not api_key:
            logger.warning("Gemini API keyκ°€ μ„¤μ •λμ§€ μ•μ•μµλ‹λ‹¤. κΈ°λ³Έ λ©”μ‹μ§€λ¥Ό λ°ν™ν•©λ‹λ‹¤.")
            self.client = None
        else:
            try:
                import google.generativeai as genai
                genai.configure(api_key=api_key)
                self.client = genai.GenerativeModel(model)
                logger.info(f"Gemini ν΄λΌμ΄μ–ΈνΈ μ΄κΈ°ν™” μ™„λ£: model={model}")
            except Exception as e:
                logger.error(f"Gemini ν΄λΌμ΄μ–ΈνΈ μ΄κΈ°ν™” μ‹¤ν¨: {e}")
                self.client = None
    
    def generate_summary(
        self, 
        top_factors: List[tuple],
        evidence_reviews: List[Dict[str, Any]],
        total_turns: int,
        category_name: str,
        product_name: str = "μ΄ μ ν’",
        dialogue_history: Optional[List[Dict[str, str]]] = None
    ) -> str:
        """μµμΆ… λ¶„μ„ μ”μ•½ μƒμ„±"""
        
        if not self.client:
            return self._get_fallback_summary(top_factors, category_name, product_name)
        
        # ν”„λ΅¬ν”„νΈ κµ¬μ„±
        prompt = self._build_prompt(top_factors, evidence_reviews, total_turns, category_name, product_name, dialogue_history)
        
        try:
            response = self.client.generate_content(
                prompt,
                generation_config={
                    "temperature": self.temperature,
                    "max_output_tokens": self.max_tokens,
                }
            )
            
            summary = response.text.strip()
            logger.info(f"Gemini μ”μ•½ μƒμ„± μ™„λ£: {len(summary)}μ")
            return summary
            
        except Exception as e:
            logger.error(f"Gemini API νΈμ¶ μ‹¤ν¨: {e}")
            return self._get_fallback_summary(top_factors, category_name, product_name)
    
    def _build_prompt(
        self,
        top_factors: List[tuple],
        evidence_reviews: List[Dict[str, Any]],
        total_turns: int,
        category_name: str,
        product_name: str,
        dialogue_history: Optional[List[Dict[str, str]]] = None
    ) -> str:
        """ν”„λ΅¬ν”„νΈ κµ¬μ„±"""
        
        # μƒμ„ μ”μΈ μ •λ¦¬
        factors_text = "\n".join([
            f"{i+1}. {factor_key} (μ μ: {score:.2f})"
            for i, (factor_key, score) in enumerate(top_factors[:5])
        ])
        
        # λ€ν™” λ‚΄μ© μ •λ¦¬
        dialogue_text = ""
        if dialogue_history:
            dialogue_lines = []
            for turn in dialogue_history:
                role = turn.get('role', '')
                text = turn.get('message', '')
                if role == 'user':
                    dialogue_lines.append(f"μ‚¬μ©μ: {text}")
                elif role == 'assistant':
                    dialogue_lines.append(f"μ–΄μ‹μ¤ν„΄νΈ: {text}")
            dialogue_text = "\n".join(dialogue_lines)
        
        # λ¨λ“  μ¦κ±° λ¦¬λ·° ν¬ν•¨ (μƒμ„ 5κ°κ°€ μ•„λ‹ μ „μ²΄)
        evidence_text = ""
        for i, rev in enumerate(evidence_reviews, 1):
            label = rev.get('label', 'NEU')
            rating = rev.get('rating', 0)
            excerpt = rev.get('excerpt', '')  # μ „μ²΄ λ¦¬λ·° λ‚΄μ© μ‚¬μ©
            evidence_text += f"{i}. [{label}] {rating}μ  - {excerpt}\n"
        
        # Prompt κµ¬μ„±
        prompt_parts = [
            "λ‹Ήμ‹ μ€ μ ν’ λ¦¬λ·° λ¶„μ„ μ „λ¬Έκ°€μ…λ‹λ‹¤.",
            "",
            "**μ ν’ μ •λ³΄**",
            f"- μΉ΄ν…κ³ λ¦¬: {category_name}",
            f"- μ ν’λ…: {product_name}",
            f"- λ¶„μ„ λ€ν™” ν„΄: {total_turns}ν„΄",
            ""
        ]
        
        if dialogue_text:
            prompt_parts.extend([
                "**λ€ν™” λ‚΄μ©**",
                dialogue_text,
                ""
            ])
        
        prompt_parts.extend([
            "**μ£Όμ” ν›„ν μ”μΈ (μƒμ„ 5κ°)**",
            factors_text,
            "",
            f"**μ¦κ±° λ¦¬λ·° μ „μ²΄ ({len(evidence_reviews)}κ°)**",
            evidence_text,
            "",
            "μ„ λ¶„μ„ κ²°κ³Όλ¥Ό λ°”νƒ•μΌλ΅ **κµ¬λ§¤μμ—κ² λ„μ›€μ΄ λλ” μµμΆ… μ”μ•½**μ„ μ‘μ„±ν•΄μ£Όμ„Έμ”.",
            "",
            "λ‹¤μ ν•μ‹μΌλ΅ μ‘μ„±:",
            "1. ν•µμ‹¬ ν›„ν μ”μΈ μ„¤λ… (2-3λ¬Έμ¥)",
            "2. κµ¬λ§¤ μ „ μ²΄ν¬ν¬μΈνΈ (3-5κ° ν•­λ©, κ° 1-2λ¬Έμ¥)",
            "3. ν• μ¤„ μ΅°μ–Έ",
            "",
            "**ν†¤μ•¤λ§¤λ„**: μΉκ·Όν•μ§€λ§ μ „λ¬Έμ , κµ¬μ²΄μ μ΄κ³  μ‹¤μ©μ ",
            "**κΈΈμ΄**: 300-500μ"
        ])
        
        prompt = "\n".join(prompt_parts)
        return prompt
    
    def _get_fallback_summary(self, top_factors: List[tuple], category_name: str, product_name: str) -> str:
        """API μ‹¤ν¨ μ‹ κΈ°λ³Έ μ”μ•½"""
        
        factors_text = ", ".join([f"{key}" for key, _ in top_factors[:3]])
        
        return f"""π” **{product_name} λ¶„μ„ μ™„λ£**

**μ£Όμ” ν›„ν μ”μΈ**: {factors_text}

μ„ μ”μΈλ“¤μ΄ μ‹¤μ  κµ¬λ§¤μλ“¤μ΄ κ°€μ¥ λ§μ΄ ν›„νν• λ¶€λ¶„μ…λ‹λ‹¤.

**κµ¬λ§¤ μ „ μ²΄ν¬ν¬μΈνΈ**:
1. ν•΄λ‹Ή μ”μΈλ“¤μ΄ λ³ΈμΈμ—κ² μ¤‘μ”ν•μ§€ ν™•μΈν•μ„Έμ”
2. λ‚®μ€ ν‰μ  λ¦¬λ·°μ—μ„ κµ¬μ²΄μ μΈ λ¶λ§ λ‚΄μ©μ„ ν™•μΈν•μ„Έμ”
3. μ μ‚¬ μ ν’κ³Ό λΉ„κµν•΄λ³΄μ„Έμ”

π’΅ **μ΅°μ–Έ**: ν›„ν μ”μΈμ„ λ―Έλ¦¬ μ•κ³  κµ¬λ§¤ν•λ©΄ μ‹¤λ§μ„ μ¤„μΌ μ μμµλ‹λ‹¤!
"""
